{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12d22220",
   "metadata": {},
   "source": [
    "# This markdown file records the training of models that did not perform as well in predicting D0-D4 values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9735b14d",
   "metadata": {},
   "source": [
    "#### Train MultiOutputRegressor(RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Load and prepare data\n",
    "# df = pd.read_csv(\"final_historical.csv\")\n",
    "df = final_historical_df\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert 'date' column to datetime if not already\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "input_cols = ['airtemp', 'baseflow', 'ev', 'rainfall', 'runoff', 'sm', 'snowfall', 'snowwater']\n",
    "target_cols = ['D0', 'D1', 'D2', 'D3', 'D4']\n",
    "\n",
    "# Step 1: Simplify Lagged Features (Keep only 1 lag feature for each input column)\n",
    "for col in input_cols:\n",
    "    df[f'{col}_lag1'] = df[col].shift(1)  # 1-month lag\n",
    "\n",
    "# Drop rows with missing values created by lagging\n",
    "df = df.dropna()\n",
    "\n",
    "# Step 2: Create Time-Aware Train/Test Split (based on date)\n",
    "X_train = df[df['date'] < '2000-01-01'][input_cols + [f'{col}_lag1' for col in input_cols]]\n",
    "y_train = df[df['date'] < '2000-01-01'][target_cols]\n",
    "X_test = df[df['date'] >= '2000-01-01'][input_cols + [f'{col}_lag1' for col in input_cols]]\n",
    "y_test = df[df['date'] >= '2000-01-01'][target_cols]\n",
    "\n",
    "# Step 3: Scale the Features (Input columns + lagged features)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 1Ô∏è‚É£ RANDOM FOREST MODEL\n",
    "rf_model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, max_depth=6, random_state=42))\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(rf_model, \"random_forest_drought.pkl\")\n",
    "\n",
    "# Evaluation\n",
    "rf_mse = mean_squared_error(y_test, rf_pred, multioutput='raw_values')\n",
    "rf_r2 = r2_score(y_test, rf_pred, multioutput='raw_values')\n",
    "rf_accuracy = 1 - np.mean(rf_mse / np.var(y_test, axis=0))\n",
    "\n",
    "print(\"üå≤ Random Forest Performance:\")\n",
    "for i, col in enumerate(target_cols):\n",
    "    print(f\"{col}: MSE = {rf_mse[i]:.4f}, R¬≤ = {rf_r2[i]:.4f}\")\n",
    "print(f\"Overall pseudo-accuracy (1 - normalized MSE): {rf_accuracy:.4f}\\n\")\n",
    "\n",
    "\n",
    "# 2Ô∏è‚É£ XGBOOST MODEL\n",
    "xgb_model = MultiOutputRegressor(XGBRegressor(n_estimators=100, learning_rate=0.05, max_depth=4, verbosity=0))\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Save model\n",
    "joblib.dump(xgb_model, \"xgboost_drought.pkl\")\n",
    "\n",
    "# Evaluation\n",
    "xgb_mse = mean_squared_error(y_test, xgb_pred, multioutput='raw_values')\n",
    "xgb_r2 = r2_score(y_test, xgb_pred, multioutput='raw_values')\n",
    "xgb_accuracy = 1 - np.mean(xgb_mse / np.var(y_test, axis=0))\n",
    "\n",
    "print(\"‚ö° XGBoost Performance:\")\n",
    "for i, col in enumerate(target_cols):\n",
    "    print(f\"{col}: MSE = {xgb_mse[i]:.4f}, R¬≤ = {xgb_r2[i]:.4f}\")\n",
    "print(f\"Overall pseudo-accuracy (1 - normalized MSE): {xgb_accuracy:.4f}\\n\")\n",
    "\n",
    "\n",
    "# üîç Visualization (XGBoost)\n",
    "for i, col in enumerate(target_cols):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(y_test[col], xgb_pred[:, i], alpha=0.5, edgecolor='k')\n",
    "    plt.plot([y_test[col].min(), y_test[col].max()],\n",
    "             [y_test[col].min(), y_test[col].max()],\n",
    "             'r--', lw=2)\n",
    "    plt.title(f\"{col} Prediction (XGBoost)\")\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea43ed7",
   "metadata": {},
   "source": [
    "## LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# --- LOAD & PREP DATA ---\n",
    "# df = pd.read_csv(\"final_historical.csv\")\n",
    "df = final_historical_df\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.dropna()\n",
    "\n",
    "input_cols = ['airtemp', 'baseflow', 'ev', 'rainfall', 'runoff', 'sm', 'snowfall', 'snowwater']\n",
    "target_cols = ['D0', 'D1', 'D2', 'D3', 'D4']\n",
    "\n",
    "# --- SCALE FEATURES ---\n",
    "scaler = StandardScaler()\n",
    "df[input_cols] = scaler.fit_transform(df[input_cols])\n",
    "\n",
    "# --- CREATE SEQUENCES ---\n",
    "def create_sequences(df, input_cols, target_cols, seq_len=12):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_len):\n",
    "        X.append(df[input_cols].iloc[i:i+seq_len].values)\n",
    "        y.append(df[target_cols].iloc[i+seq_len].values)  # predict drought at next time step\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SEQ_LEN = 12\n",
    "X_seq, y_seq = create_sequences(df, input_cols, target_cols, SEQ_LEN)\n",
    "\n",
    "# --- TIME-AWARE SPLIT ---\n",
    "split_idx = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "# --- LSTM MODEL ---\n",
    "model = Sequential([\n",
    "    LSTM(64, input_shape=(SEQ_LEN, len(input_cols)), return_sequences=False),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(target_cols))  # one output per drought class\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "early_stop = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "# --- TRAIN ---\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stop],\n",
    "                    verbose=1)\n",
    "\n",
    "# --- EVALUATE ---\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')\n",
    "r2 = r2_score(y_test, y_pred, multioutput='raw_values')\n",
    "pseudo_acc = 1 - np.mean(mse / np.var(y_test, axis=0))\n",
    "\n",
    "print(\"üìâ LSTM Performance:\")\n",
    "for i, col in enumerate(target_cols):\n",
    "    print(f\"{col}: MSE = {mse[i]:.4f}, R¬≤ = {r2[i]:.4f}\")\n",
    "print(f\"Overall pseudo-accuracy (1 - normalized MSE): {pseudo_acc:.4f}\\n\")\n",
    "\n",
    "# --- PLOT PREDICTIONS ---\n",
    "for i, col in enumerate(target_cols):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(y_test[:, i], label=\"Actual\")\n",
    "    plt.plot(y_pred[:, i], label=\"Predicted\")\n",
    "    plt.title(f\"{col} Prediction (LSTM)\")\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ec0fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === LOAD & PREPROCESS ===\n",
    "# df = pd.read_csv(\"final_historical.csv\")\n",
    "df = final_historical_df\n",
    "df.dropna(inplace=True)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "input_cols = ['airtemp', 'baseflow', 'ev', 'rainfall', 'runoff', 'sm', 'snowfall', 'snowwater']\n",
    "target_cols = ['D0', 'D1', 'D2', 'D3', 'D4']\n",
    "SEQ_LEN = 3  # how many months of memory\n",
    "\n",
    "# === SCALING ===\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "df[input_cols] = scaler_X.fit_transform(df[input_cols])\n",
    "df[target_cols] = scaler_y.fit_transform(df[target_cols])\n",
    "\n",
    "# === CREATE SEQUENCES ===\n",
    "def create_sequences(df, input_cols, target_cols, seq_len):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_len):\n",
    "        X.append(df[input_cols].iloc[i:i+seq_len].values)\n",
    "        y.append(df[target_cols].iloc[i+seq_len].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_seq, y_seq = create_sequences(df, input_cols, target_cols, SEQ_LEN)\n",
    "\n",
    "# === TIME-AWARE SPLIT ===\n",
    "dates = df['date'].iloc[SEQ_LEN:].reset_index(drop=True)\n",
    "split_date = pd.Timestamp('2000-01-01')\n",
    "train_idx = dates < split_date\n",
    "test_idx = dates >= split_date\n",
    "\n",
    "X_train, y_train = X_seq[train_idx], y_seq[train_idx]\n",
    "X_test, y_test = X_seq[test_idx], y_seq[test_idx]\n",
    "\n",
    "# === MODEL BUILDING ===\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(SEQ_LEN, len(input_cols))),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(target_cols))\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mae')\n",
    "\n",
    "early_stop = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "# === TRAINING ===\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# === PREDICTION & INVERSE SCALING ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_inv = scaler_y.inverse_transform(y_pred)\n",
    "y_test_inv = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "# === EVALUATION ===\n",
    "mse = mean_squared_error(y_test_inv, y_pred_inv, multioutput='raw_values')\n",
    "r2 = r2_score(y_test_inv, y_pred_inv, multioutput='raw_values')\n",
    "pseudo_acc = 1 - np.mean(mse / np.var(y_test_inv, axis=0))\n",
    "\n",
    "print(\"üìâ LSTM Performance:\")\n",
    "for i, col in enumerate(target_cols):\n",
    "    print(f\"{col}: MSE = {mse[i]:.4f}, R¬≤ = {r2[i]:.4f}\")\n",
    "print(f\"Overall pseudo-accuracy (1 - normalized MSE): {pseudo_acc:.4f}\")\n",
    "\n",
    "# === PLOT RESULTS ===\n",
    "for i, col in enumerate(target_cols):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(y_test_inv[:, i], y_pred_inv[:, i], alpha=0.5, edgecolor='k')\n",
    "    plt.plot([y_test_inv[:, i].min(), y_test_inv[:, i].max()],\n",
    "             [y_test_inv[:, i].min(), y_test_inv[:, i].max()],\n",
    "             'r--', lw=2)\n",
    "    plt.title(f\"{col} Prediction (LSTM)\")\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Dropout, Attention, LayerNormalization, Concatenate\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === LOAD & CLEAN ===\n",
    "# df = pd.read_csv(\"final_historical.csv\")\n",
    "df = final_historical_df\n",
    "df.dropna(inplace=True)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "input_cols = ['airtemp', 'baseflow', 'ev', 'rainfall', 'runoff', 'sm', 'snowfall', 'snowwater']\n",
    "target_cols = ['D0', 'D1', 'D2', 'D3', 'D4']\n",
    "SEQ_LEN = 6          # number of months to look back\n",
    "PRED_HORIZON = 3     # number of future steps to predict\n",
    "\n",
    "# === SCALE ===\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "df[input_cols] = scaler_X.fit_transform(df[input_cols])\n",
    "df[target_cols] = scaler_y.fit_transform(df[target_cols])\n",
    "\n",
    "# === SEQUENCE CREATION ===\n",
    "def create_multi_horizon_sequences(df, input_cols, target_cols, seq_len, pred_horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_len - pred_horizon + 1):\n",
    "        X.append(df[input_cols].iloc[i:i+seq_len].values)\n",
    "        y.append(df[target_cols].iloc[i+seq_len:i+seq_len+pred_horizon].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_seq, y_seq = create_multi_horizon_sequences(df, input_cols, target_cols, SEQ_LEN, PRED_HORIZON)\n",
    "\n",
    "# === TIME-AWARE SPLIT ===\n",
    "dates = df['date'].iloc[SEQ_LEN:SEQ_LEN + len(y_seq)].reset_index(drop=True)\n",
    "split_date = pd.Timestamp(\"2000-01-01\")\n",
    "train_idx = dates < split_date\n",
    "test_idx = dates >= split_date\n",
    "\n",
    "X_train, y_train = X_seq[train_idx], y_seq[train_idx]\n",
    "X_test, y_test = X_seq[test_idx], y_seq[test_idx]\n",
    "\n",
    "# === MODEL WITH ATTENTION ===\n",
    "input_layer = Input(shape=(SEQ_LEN, len(input_cols)))\n",
    "x = GRU(128, return_sequences=True)(input_layer)\n",
    "x = Dropout(0.2)(x)\n",
    "x = LayerNormalization()(x)\n",
    "\n",
    "# Attention block\n",
    "query = Dense(64)(x)\n",
    "key = Dense(64)(x)\n",
    "value = Dense(64)(x)\n",
    "attn_out = Attention(use_scale=True)([query, value, key])\n",
    "attn_out = LayerNormalization()(attn_out)\n",
    "\n",
    "x = Concatenate()([x, attn_out])\n",
    "x = GRU(64)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "output = Dense(PRED_HORIZON * len(target_cols))(x)\n",
    "output = tf.keras.layers.Reshape((PRED_HORIZON, len(target_cols)))(output)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output)\n",
    "model.compile(optimizer=Adam(0.001), loss='mae')\n",
    "\n",
    "# === TRAIN ===\n",
    "early_stop = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(patience=5, factor=0.5, verbose=1)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# === PREDICT & EVALUATE ===\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_inv = scaler_y.inverse_transform(y_pred.reshape(-1, len(target_cols))).reshape(-1, PRED_HORIZON, len(target_cols))\n",
    "y_test_inv = scaler_y.inverse_transform(y_test.reshape(-1, len(target_cols))).reshape(-1, PRED_HORIZON, len(target_cols))\n",
    "\n",
    "# Evaluate at each future time step\n",
    "print(\"üìâ Multi-Horizon GRU + Attention Performance:\")\n",
    "for step in range(PRED_HORIZON):\n",
    "    mse = mean_squared_error(y_test_inv[:, step], y_pred_inv[:, step], multioutput='raw_values')\n",
    "    r2 = r2_score(y_test_inv[:, step], y_pred_inv[:, step], multioutput='raw_values')\n",
    "    pseudo_acc = 1 - np.mean(mse / np.var(y_test_inv[:, step], axis=0))\n",
    "    print(f\"üîÆ Step {step+1}:\")\n",
    "    for i, col in enumerate(target_cols):\n",
    "        print(f\"  {col}: MSE = {mse[i]:.4f}, R¬≤ = {r2[i]:.4f}\")\n",
    "    print(f\"  Overall pseudo-accuracy: {pseudo_acc:.4f}\\n\")\n",
    "\n",
    "# === PLOTS ===\n",
    "for i, col in enumerate(target_cols):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(y_test_inv[:, 0, i], y_pred_inv[:, 0, i], alpha=0.5, edgecolor='k')\n",
    "    plt.plot([y_test_inv[:, 0, i].min(), y_test_inv[:, 0, i].max()],\n",
    "             [y_test_inv[:, 0, i].min(), y_test_inv[:, 0, i].max()],\n",
    "             'r--', lw=2)\n",
    "    plt.title(f\"{col} (Step 1 Forecast)\")\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf3e22",
   "metadata": {},
   "source": [
    "### LSTM run that worked well but did not contribute to our results/conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8272a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Preprocessing and Feature Engineering\n",
    "# Assuming final_historical_df is your dataframe\n",
    "final_historical_df['date'] = pd.to_datetime(final_historical_df['date'])\n",
    "\n",
    "# Create Lag Features (1-step lag for D0, D1, D2, D3, D4)\n",
    "for col in ['D0', 'D1', 'D2', 'D3', 'D4']:\n",
    "    for lag in range(1, 4):  # Lag 1, 2, 3\n",
    "        final_historical_df[f'{col}_lag{lag}'] = final_historical_df[col].shift(lag)\n",
    "\n",
    "# Create Rolling Mean and Std Features (using a window of 7 days, for example)\n",
    "for col in ['D0', 'D1', 'D2', 'D3', 'D4']:\n",
    "    final_historical_df[f'{col}_rolling_mean'] = final_historical_df[col].rolling(window=7).mean()\n",
    "    final_historical_df[f'{col}_rolling_std'] = final_historical_df[col].rolling(window=7).std()\n",
    "\n",
    "# Drop NaN values generated by shifting and rolling operations\n",
    "final_historical_df = final_historical_df.dropna()\n",
    "\n",
    "# Step 2: Prepare Features and Targets\n",
    "features = final_historical_df.drop(columns=['date', 'D0', 'D1', 'D2', 'D3', 'D4'])\n",
    "target = final_historical_df[['D0', 'D1', 'D2', 'D3', 'D4']]\n",
    "\n",
    "# Normalize Features and Targets\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = feature_scaler.fit_transform(features)\n",
    "y_scaled = target_scaler.fit_transform(target)\n",
    "\n",
    "# Step 3: Create Sequences for LSTM\n",
    "SEQ_LENGTH = 12 # 12 time steps (e.g., 12 weeks of data)\n",
    "# 12 = all r^2 are above 0 (d3,4 improved)\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(len(X_scaled) - SEQ_LENGTH):\n",
    "    X_seq.append(X_scaled[i:i+SEQ_LENGTH])  # Sequence of features\n",
    "    y_seq.append(y_scaled[i+SEQ_LENGTH])  # Next time step prediction\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# Step 4: Train/Test Split\n",
    "split_idx = int(0.8 * len(X_seq))  # 80% train, 20% test\n",
    "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "# Step 5: Build and Train the LSTM Model\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LENGTH, X_train.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dense(5)  # Output size matches D0, D1, D2, D3, D4\n",
    "])\n",
    "\n",
    "learning_rate = 0.05\n",
    "# Compile the model with the custom learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions and true labels to original scale\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_orig = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculate MAE and R¬≤ for each drought level\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "for i, category in enumerate(['D0', 'D1', 'D2', 'D3', 'D4']):\n",
    "    mae = mean_absolute_error(y_test_orig[:, i], y_pred[:, i])\n",
    "    r2 = r2_score(y_test_orig[:, i], y_pred[:, i])\n",
    "    print(f\"{category} - MAE: {mae:.4f}, R¬≤: {r2:.4f}\")\n",
    "    \n",
    "    # Plot predictions vs true values\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_test_orig[:, i], label='True')\n",
    "    plt.plot(y_pred[:, i], label='Predicted')\n",
    "    plt.title(f\"{category} - True vs Predicted\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Percentage Area\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333106e3",
   "metadata": {},
   "source": [
    "### Training each D0-D4 label separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155620f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, Dropout, Attention, LayerNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === LOAD & CLEAN ===\n",
    "# df = pd.read_csv(\"final_historical.csv\")\n",
    "df = final_historical_df\n",
    "df.dropna(inplace=True)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "input_cols = ['airtemp', 'baseflow', 'ev', 'rainfall', 'runoff', 'sm', 'snowfall', 'snowwater']\n",
    "target_cols = ['D0', 'D1', 'D2', 'D3', 'D4']\n",
    "SEQ_LEN = 6\n",
    "PRED_HORIZON = 3  # multi-horizon (e.g., next 3 months)\n",
    "\n",
    "# === SCALE INPUT FEATURES ===\n",
    "scaler_X = StandardScaler()\n",
    "df[input_cols] = scaler_X.fit_transform(df[input_cols])\n",
    "\n",
    "# === CREATE SEQUENCES FOR EACH TARGET ===\n",
    "def create_sequences(df, input_cols, target_col, seq_len, pred_horizon):\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_len - pred_horizon + 1):\n",
    "        X.append(df[input_cols].iloc[i:i+seq_len].values)\n",
    "        y.append(df[target_col].iloc[i+seq_len:i+seq_len+pred_horizon].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# === TIME SPLIT HELPER ===\n",
    "def time_aware_split(X, y, dates, split_date):\n",
    "    split_mask = dates < split_date\n",
    "    return X[split_mask], y[split_mask], X[~split_mask], y[~split_mask]\n",
    "\n",
    "# === TRAIN EACH MODEL ===\n",
    "for target_col in target_cols:\n",
    "    print(f\"\\nüöÄ Training for {target_col}...\\n\")\n",
    "    \n",
    "    # Scale the target independently\n",
    "    scaler_y = StandardScaler()\n",
    "    df[target_col] = scaler_y.fit_transform(df[[target_col]])\n",
    "    \n",
    "    X_seq, y_seq = create_sequences(df, input_cols, target_col, SEQ_LEN, PRED_HORIZON)\n",
    "    dates = df['date'].iloc[SEQ_LEN:SEQ_LEN + len(y_seq)].reset_index(drop=True)\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = time_aware_split(X_seq, y_seq, dates, pd.Timestamp(\"2000-01-01\"))\n",
    "    \n",
    "    # === MODEL ===\n",
    "    input_layer = Input(shape=(SEQ_LEN, len(input_cols)))\n",
    "    x = GRU(64, return_sequences=True)(input_layer)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # Attention layer\n",
    "    query = Dense(32)(x)\n",
    "    key = Dense(32)(x)\n",
    "    value = Dense(32)(x)\n",
    "    attn_out = Attention(use_scale=True)([query, value, key])\n",
    "    x = tf.keras.layers.Concatenate()([x, attn_out])\n",
    "\n",
    "    x = GRU(32)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    output = Dense(PRED_HORIZON)(x)  # One output per month\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(0.001), loss='mae')\n",
    "\n",
    "    early_stop = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(patience=5, factor=0.5, verbose=1)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_split=0.1,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stop, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Predict + Inverse Scale\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_inv = scaler_y.inverse_transform(y_pred)\n",
    "    y_test_inv = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "    # === EVALUATION ===\n",
    "    mse = mean_squared_error(y_test_inv, y_pred_inv, multioutput='raw_values')\n",
    "    r2 = r2_score(y_test_inv, y_pred_inv, multioutput='raw_values')\n",
    "    pseudo_acc = 1 - np.mean(mse / np.var(y_test_inv, axis=0))\n",
    "\n",
    "    print(f\"üìâ {target_col} Performance:\")\n",
    "    for i in range(PRED_HORIZON):\n",
    "        print(f\"Step {i+1}: MSE = {mse[i]:.4f}, R¬≤ = {r2[i]:.4f}\")\n",
    "    print(f\"Overall pseudo-accuracy: {pseudo_acc:.4f}\")\n",
    "\n",
    "    # === PLOT FIRST STEP ===\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(y_test_inv[:, 0], y_pred_inv[:, 0], alpha=0.5, edgecolor='k')\n",
    "    plt.plot([y_test_inv[:, 0].min(), y_test_inv[:, 0].max()],\n",
    "             [y_test_inv[:, 0].min(), y_test_inv[:, 0].max()],\n",
    "             'r--', lw=2)\n",
    "    plt.title(f\"{target_col} Prediction (Step 1)\")\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b88560a",
   "metadata": {},
   "source": [
    "### LSTM w/ independent drought levels as ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca40413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "features = final_historical_df.drop(columns=['date', 'D0', 'D1', 'D2', 'D3', 'D4'])\n",
    "target = final_historical_df[['D0', 'D1', 'D2', 'D3', 'D4']]  # Multi-output regression\n",
    "\n",
    "# Ensure proper data preparation\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Check if required columns exist\n",
    "required_cols = ['date', 'D0', 'D1', 'D2', 'D3', 'D4']\n",
    "missing_cols = [col for col in required_cols if col not in final_historical_df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing columns in final_df: {missing_cols}\")\n",
    "\n",
    "# Add temporal features\n",
    "pd.to_datetime(final_historical_df['date'])\n",
    "\n",
    "# final_historical_df['date'] = final_historical_df['date'][:-2]\n",
    "\n",
    "# Scale features and target\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = feature_scaler.fit_transform(features)\n",
    "y_scaled = target_scaler.fit_transform(target)\n",
    "\n",
    "print(f\"Features shape: {X_scaled.shape}, Target shape: {y_scaled.shape}\")\n",
    "\n",
    "# Create sequences for LSTM\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i+seq_length])  # Predict next step after sequence\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "SEQ_LENGTH = 12  # Lookback period (12 weeks)\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQ_LENGTH)\n",
    "\n",
    "print(f\"Sequence shapes - X_seq: {X_seq.shape}, y_seq: {y_seq.shape}\")\n",
    "\n",
    "# Split sequences into train and test sets\n",
    "split_idx = int(0.8 * len(X_seq))  # 80% train, 20% test\n",
    "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "print(f\"Train shapes - X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Test shapes - X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Define the LSTM model architecture\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LENGTH, X_train.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dense(5)  # Output size matches the number of drought categories (D0-D4)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model with early stopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions and true labels to original scale\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_orig = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "print(f\"Prediction shape: {y_pred.shape}, True labels shape: {y_test_orig.shape}\")\n",
    "\n",
    "# Evaluate using MAE for each drought category (D0-D4)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "for i, category in enumerate(['D0', 'D1', 'D2', 'D3', 'D4']):\n",
    "    mae = mean_absolute_error(y_test_orig[:, i], y_pred[:, i])\n",
    "    print(f\"MAE for {category}: {mae:.4f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot predictions vs true values for D0 category as an example\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_orig[:, 0], label='True D0')\n",
    "plt.plot(y_pred[:, 0], label='Predicted D0')\n",
    "plt.title('Predictions vs True Values for D0')\n",
    "plt.xlabel('Time Steps')\n",
    "plt.ylabel('Percentage Area')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Flatten sequences for Random Forest (reshape to 2D)\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Train Random Forest Regressor\n",
    "rf_model = RandomForestRegressor(n_estimators=100)\n",
    "rf_model.fit(X_train_flat, y_train)\n",
    "\n",
    "# Predict and evaluate RF model\n",
    "rf_pred = rf_model.predict(X_test_flat)\n",
    "rf_mae = mean_absolute_error(y_test, rf_pred)\n",
    "\n",
    "print(f\"Random Forest MAE: {rf_mae:.4f}\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Convert predictions and true values to categorical labels\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_labels, y_pred_labels)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['D0', 'D1', 'D2'], yticklabels=['D0', 'D1', 'D2'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44114990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Setup\n",
    "SEQ_LENGTH = 12\n",
    "categories = ['D0', 'D1', 'D2', 'D3', 'D4']\n",
    "results = {}\n",
    "\n",
    "# Convert date column\n",
    "final_historical_df['date'] = pd.to_datetime(final_historical_df['date'])\n",
    "\n",
    "# Feature matrix (constant across models)\n",
    "features = final_historical_df.drop(columns=['date', 'D0', 'D1', 'D2', 'D3', 'D4'])\n",
    "feature_scaler = MinMaxScaler()\n",
    "X_scaled = feature_scaler.fit_transform(features)\n",
    "\n",
    "# Sequence creation function\n",
    "def create_sequences(X, y, seq_length):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - seq_length):\n",
    "        X_seq.append(X[i:i+seq_length])\n",
    "        y_seq.append(y[i+seq_length])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Loop over drought categories\n",
    "for cat in categories:\n",
    "    print(f\"\\nüöÄ Training model for {cat}\")\n",
    "\n",
    "    # Prepare target\n",
    "    y = final_historical_df[[cat]].values\n",
    "    target_scaler = MinMaxScaler()\n",
    "    y_scaled = target_scaler.fit_transform(y)\n",
    "\n",
    "    # Create sequences\n",
    "    X_seq, y_seq = create_sequences(X_scaled, y_scaled, SEQ_LENGTH)\n",
    "\n",
    "    # Train-test split\n",
    "    split_idx = int(0.8 * len(X_seq))\n",
    "    X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "    y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "    # Build LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(64, return_sequences=True, input_shape=(SEQ_LENGTH, X_train.shape[2])),\n",
    "        Dropout(0.3),\n",
    "        LSTM(32),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Predict\n",
    "    y_pred_scaled = model.predict(X_test)\n",
    "    y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "    y_test_orig = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "    # Metrics\n",
    "    mae = mean_absolute_error(y_test_orig, y_pred)\n",
    "    mse = mean_squared_error(y_test_orig, y_pred)\n",
    "    r2 = r2_score(y_test_orig, y_pred)\n",
    "\n",
    "    print(f\"{cat} MAE: {mae:.4f}, MSE: {mse:.4f}, R¬≤: {r2:.4f}\")\n",
    "\n",
    "    # Save results\n",
    "    results[cat] = {\n",
    "        'true': y_test_orig.flatten(),\n",
    "        'pred': y_pred.flatten(),\n",
    "        'mae': mae,\n",
    "        'mse': mse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_test_orig, label='True')\n",
    "    plt.plot(y_pred, label='Predicted')\n",
    "    plt.title(f'{cat} Prediction')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Percentage Area')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Random Forest comparison\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=100)\n",
    "    rf.fit(X_train_flat, y_train.ravel())\n",
    "    rf_pred = target_scaler.inverse_transform(rf.predict(X_test_flat).reshape(-1, 1))\n",
    "\n",
    "    rf_mae = mean_absolute_error(y_test_orig, rf_pred)\n",
    "    rf_r2 = r2_score(y_test_orig, rf_pred)\n",
    "\n",
    "    print(f\"üå≥ Random Forest {cat} MAE: {rf_mae:.4f}, R¬≤: {rf_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ba5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Replace this with your actual dataframe\n",
    "# final_historical_df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Drop any rows with missing values\n",
    "final_historical_df = final_historical_df.dropna()\n",
    "\n",
    "# Features to use\n",
    "X_raw = final_historical_df.drop(columns=['date', 'D0', 'D1', 'D2', 'D3', 'D4'])\n",
    "\n",
    "# Store results\n",
    "metrics = {}\n",
    "\n",
    "# Train a separate model for each drought level\n",
    "for target_col in ['D0', 'D1', 'D2', 'D3', 'D4']:\n",
    "    print(f\"\\nTraining model for {target_col}...\")\n",
    "\n",
    "    # Target column\n",
    "    y_raw = final_historical_df[target_col].values.reshape(-1, 1)\n",
    "\n",
    "    # Scale features and target\n",
    "    X_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    X = X_scaler.fit_transform(X_raw)\n",
    "    y = y_scaler.fit_transform(y_raw)\n",
    "\n",
    "    # Time-aware split (or shuffle=True for random)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Define small neural network\n",
    "    model = Sequential([\n",
    "        Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
    "              validation_split=0.2, verbose=0)\n",
    "\n",
    "    # Predict and inverse transform\n",
    "    y_pred_scaled = model.predict(X_test)\n",
    "    y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "    y_true = y_scaler.inverse_transform(y_test)\n",
    "\n",
    "    # Evaluate\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    metrics[target_col] = {'mae': mae, 'r2': r2}\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "for cat, m in metrics.items():\n",
    "    print(f\"{cat} - MAE: {m['mae']:.4f}, R¬≤: {m['r2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bebc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "\n",
    "# Load and clean your dataframe\n",
    "# final_historical_df = pd.read_csv('your_data.csv')\n",
    "final_historical_df = final_historical_df.dropna()\n",
    "\n",
    "# Features (drop date + target drought levels)\n",
    "X_raw = final_historical_df.drop(columns=['date', 'D0', 'D1', 'D2', 'D3', 'D4'])\n",
    "\n",
    "# Targets (D0-D4)\n",
    "y_raw = final_historical_df[['D0', 'D1', 'D2', 'D3', 'D4']].values\n",
    "\n",
    "# Scale features and targets\n",
    "X_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X = X_scaler.fit_transform(X_raw)\n",
    "y = y_scaler.fit_transform(y_raw)\n",
    "\n",
    "# Train-test split (time-aware)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Define a more complex neural network architecture\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(5)  # 5 outputs: D0, D1, D2, D3, D4\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
    "                    validation_split=0.2, verbose=0)\n",
    "\n",
    "# Predict and inverse scale\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "y_true = y_scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# Display overall MAE and R¬≤\n",
    "print(f\"Overall MAE: {mae:.4f}\")\n",
    "print(f\"Overall R¬≤: {r2:.4f}\")\n",
    "\n",
    "# Evaluate per drought level\n",
    "for i, level in enumerate(['D0', 'D1', 'D2', 'D3', 'D4']):\n",
    "    mae_level = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "    r2_level = r2_score(y_true[:, i], y_pred[:, i])\n",
    "    print(f\"{level} - MAE: {mae_level:.4f}, R¬≤: {r2_level:.4f}\")\n",
    "\n",
    "    # Plot for each drought level\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(y_true[:, i], label='True')\n",
    "    plt.plot(y_pred[:, i], label='Predicted')\n",
    "    plt.title(f\"{level} - True vs Predicted\")\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Percentage Area\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# # Define multi-output neural network\n",
    "# model = Sequential([\n",
    "#     Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "#     Dense(32, activation='relu'),\n",
    "#     Dense(16, activation='relu'),\n",
    "#     Dense(5)  # 5 outputs: D0, D1, D2, D3, D4\n",
    "# ])\n",
    "\n",
    "# model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# # Train the model\n",
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
    "#                     validation_split=0.2, verbose=0)\n",
    "\n",
    "# # Predict and inverse scale\n",
    "# y_pred_scaled = model.predict(X_test)\n",
    "# y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "# y_true = y_scaler.inverse_transform(y_test)\n",
    "\n",
    "# # Calculate evaluation metrics\n",
    "# mae = mean_absolute_error(y_true, y_pred)\n",
    "# r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "# # Display overall MAE and R¬≤\n",
    "# print(f\"Overall MAE: {mae:.4f}\")\n",
    "# print(f\"Overall R¬≤: {r2:.4f}\")\n",
    "\n",
    "# # Evaluate per drought level\n",
    "# for i, level in enumerate(['D0', 'D1', 'D2', 'D3', 'D4']):\n",
    "#     mae_level = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "#     r2_level = r2_score(y_true[:, i], y_pred[:, i])\n",
    "#     print(f\"{level} - MAE: {mae_level:.4f}, R¬≤: {r2_level:.4f}\")\n",
    "\n",
    "#     # Plot for each drought level\n",
    "#     plt.figure(figsize=(10, 4))\n",
    "#     plt.plot(y_true[:, i], label='True')\n",
    "#     plt.plot(y_pred[:, i], label='Predicted')\n",
    "#     plt.title(f\"{level} - True vs Predicted\")\n",
    "#     plt.xlabel(\"Time Step\")\n",
    "#     plt.ylabel(\"Percentage Area\")\n",
    "#     plt.legend()\n",
    "#     plt.grid(True)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c37d7",
   "metadata": {},
   "source": [
    "Failed D4 model -- bad predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d820cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.losses import Huber\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming final_historical_df is your dataframe\n",
    "final_historical_df['date'] = pd.to_datetime(final_historical_df['date'])\n",
    "\n",
    "# Create Lag Features (1-step lag for D0, D1, D2, D3, D4)\n",
    "for col in ['D0', 'D1', 'D2', 'D3', 'D4']:\n",
    "    for lag in range(1, 4):  # Lag 1, 2, 3\n",
    "        final_historical_df[f'{col}_lag{lag}'] = final_historical_df[col].shift(lag)\n",
    "\n",
    "# Create Rolling Mean and Std Features (using a window of 7 days, for example)\n",
    "for col in ['D0', 'D1', 'D2', 'D3', 'D4']:\n",
    "    final_historical_df[f'{col}_rolling_mean'] = final_historical_df[col].rolling(window=7).mean()\n",
    "    final_historical_df[f'{col}_rolling_std'] = final_historical_df[col].rolling(window=7).std()\n",
    "\n",
    "# Drop NaN values generated by shifting and rolling operations\n",
    "final_historical_df = final_historical_df.dropna()\n",
    "\n",
    "# Prepare Features and Target specifically for D4\n",
    "features = final_historical_df.drop(columns=['date', 'D0', 'D1', 'D2', 'D3', 'D4'])\n",
    "target = final_historical_df[['D4']]\n",
    "\n",
    "# Normalize Features and Target\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = feature_scaler.fit_transform(features)\n",
    "y_scaled = target_scaler.fit_transform(target)\n",
    "\n",
    "# Create Sequences for LSTM\n",
    "SEQ_LENGTH = 12  # 12 months of data for time series forecasting\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(len(X_scaled) - SEQ_LENGTH):\n",
    "    X_seq.append(X_scaled[i:i+SEQ_LENGTH])  # Sequence of features\n",
    "    y_seq.append(y_scaled[i+SEQ_LENGTH])  # Next time step prediction for D4\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# Train/Test Split\n",
    "split_idx = int(0.8 * len(X_seq))  # 80% train, 20% test\n",
    "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "# Build and Train the LSTM Model for D4\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LENGTH, X_train.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dense(1)  # Output for D4 (single output regression task)\n",
    "])\n",
    "\n",
    "# Compile the model with Huber loss and Adam optimizer\n",
    "model.compile(optimizer='adam', loss=Huber(), metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions and true labels to original scale\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_orig = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculate MAE and R¬≤ for D4\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(y_test_orig, y_pred)\n",
    "r2 = r2_score(y_test_orig, y_pred)\n",
    "print(f\"D4 - MAE: {mae:.4f}, R¬≤: {r2:.4f}\")\n",
    "\n",
    "# Plot predictions vs true values for D4\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(y_test_orig, label='True')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.title(\"D4 - True vs Predicted\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Percentage Area\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76422490",
   "metadata": {},
   "source": [
    "Just D4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b14268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.losses import Huber\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming final_historical_df is your dataframe\n",
    "final_historical_df['date'] = pd.to_datetime(final_historical_df['date'])\n",
    "\n",
    "# Create Lag Features (1-step lag for D0, D1, D2, D3, D4)\n",
    "for col in ['D0', 'D1', 'D2', 'D3', 'D4']:\n",
    "    for lag in range(1, 4):  # Lag 1, 2, 3\n",
    "        final_historical_df[f'{col}_lag{lag}'] = final_historical_df[col].shift(lag)\n",
    "\n",
    "# Create Rolling Mean and Std Features (using a window of 7 days, for example)\n",
    "for col in ['D0', 'D1', 'D2', 'D3', 'D4']:\n",
    "    final_historical_df[f'{col}_rolling_mean'] = final_historical_df[col].rolling(window=7).mean()\n",
    "    final_historical_df[f'{col}_rolling_std'] = final_historical_df[col].rolling(window=7).std()\n",
    "\n",
    "# Drop NaN values generated by shifting and rolling operations\n",
    "final_historical_df = final_historical_df.dropna()\n",
    "\n",
    "# Prepare Features and Target specifically for D4\n",
    "features = final_historical_df.drop(columns=['date', 'D0', 'D1', 'D2', 'D3', 'D4'])\n",
    "target = final_historical_df[['D4']]\n",
    "\n",
    "# Normalize Features and Target\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled = feature_scaler.fit_transform(features)\n",
    "y_scaled = target_scaler.fit_transform(target)\n",
    "\n",
    "# Create Sequences for LSTM\n",
    "SEQ_LENGTH = 12  # 12 months of data for time series forecasting\n",
    "\n",
    "X_seq, y_seq = [], []\n",
    "for i in range(len(X_scaled) - SEQ_LENGTH):\n",
    "    X_seq.append(X_scaled[i:i+SEQ_LENGTH])  # Sequence of features\n",
    "    y_seq.append(y_scaled[i+SEQ_LENGTH])  # Next time step prediction for D4\n",
    "\n",
    "X_seq = np.array(X_seq)\n",
    "y_seq = np.array(y_seq)\n",
    "\n",
    "# Train/Test Split\n",
    "split_idx = int(0.8 * len(X_seq))  # 80% train, 20% test\n",
    "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]\n",
    "\n",
    "# Build and Train the LSTM Model for D4\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(SEQ_LENGTH, X_train.shape[2])),\n",
    "    Dropout(0.3),\n",
    "    LSTM(32),\n",
    "    Dense(1)  # Output for D4 (single output regression task)\n",
    "])\n",
    "\n",
    "# Compile the model with Huber loss and Adam optimizer\n",
    "model.compile(optimizer='adam', loss=Huber(), metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=[EarlyStopping(patience=5, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate the Model\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions and true labels to original scale\n",
    "y_pred = target_scaler.inverse_transform(y_pred_scaled)\n",
    "y_test_orig = target_scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculate MAE and R¬≤ for D4\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(y_test_orig, y_pred)\n",
    "r2 = r2_score(y_test_orig, y_pred)\n",
    "print(f\"D4 - MAE: {mae:.4f}, R¬≤: {r2:.4f}\")\n",
    "\n",
    "# Plot predictions vs true values for D4\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(y_test_orig, label='True')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.title(\"D4 - True vs Predicted\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Percentage Area\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054bff1f",
   "metadata": {},
   "source": [
    "### Random Forest and XGBoost Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc136ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Feature Engineering\n",
    "df = final_historical_df.copy()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Create lag features\n",
    "for col in ['D0', 'D1', 'D2', 'D3', 'D4']:\n",
    "    for lag in range(1, 4):\n",
    "        df[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "\n",
    "# Create rolling stats (7-day window)\n",
    "for col in ['D0', 'D1', 'D2', 'D3', 'D4']:\n",
    "    df[f'{col}_roll_mean'] = df[col].rolling(window=7).mean()\n",
    "    df[f'{col}_roll_std'] = df[col].rolling(window=7).std()\n",
    "\n",
    "# Drop NaNs caused by lag and rolling\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Step 2: Features and Targets\n",
    "X = df.drop(columns=['date', 'D0', 'D1', 'D2', 'D3', 'D4'])\n",
    "y = df[['D0', 'D1', 'D2', 'D3', 'D4']]\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Time-aware split (80% train, 20% test)\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X_scaled[:split_idx], X_scaled[split_idx:]\n",
    "y_train, y_test = y[:split_idx].values, y[split_idx:].values\n",
    "\n",
    "# Step 3: Fit and Evaluate Models\n",
    "results = {}\n",
    "\n",
    "for model_name, model_class in {\n",
    "    'RandomForest': RandomForestRegressor,\n",
    "    'XGBoost': XGBRegressor\n",
    "}.items():\n",
    "    \n",
    "    print(f\"\\n====== {model_name} Results ======\")\n",
    "    model_results = {}\n",
    "    \n",
    "    for i, target in enumerate(['D0', 'D1', 'D2', 'D3', 'D4']):\n",
    "        model = model_class(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train[:, i])\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test[:, i], y_pred)\n",
    "        r2 = r2_score(y_test[:, i], y_pred)\n",
    "        model_results[target] = {'MAE': mae, 'R2': r2}\n",
    "        \n",
    "        print(f\"{target} - MAE: {mae:.4f}, R¬≤: {r2:.4f}\")\n",
    "        \n",
    "        # Plot predictions\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(y_test[:, i], label='True')\n",
    "        plt.plot(y_pred, label='Predicted')\n",
    "        plt.title(f\"{model_name} - {target}\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"Percentage Area\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    results[model_name] = model_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
